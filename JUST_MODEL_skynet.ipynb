{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JUST MODEL-skynet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kprxDKlbMxue"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ0IMxzQuIFQ",
        "outputId": "cb5ec3e6-15c4-437a-8b02-4c8afb8816c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%load_ext tensorboard\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.python.keras.models import *\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.optimizers import *\n",
        "\n",
        "# from keras.models import *\n",
        "# from keras.layers import *\n",
        "# from keras.optimizers import *\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorboard import notebook\n",
        "\n",
        "#from tensorflow.contrib.learn.python.learn.datasets.mnist import extract_images, extract_labels\n",
        "\n",
        "print('done imports')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "done imports\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "done imports\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3HkhOb_KICw"
      },
      "source": [
        "# https://www.depends-on-the-definition.com/unet-keras-segmenting-images/\n",
        "# https://github.com/zhixuhao/unet/blob/master/model.py\n",
        "# https://ai.googleblog.com/2020/01/using-machine-learning-to-nowcast.html\n",
        "# Input -> 1Basic -> 7 DownSample -> 1Basic -> 7 UpSample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrSQMfGCv9C2"
      },
      "source": [
        "# LOAD GOOGLE DRIVE IMAGES and populate inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSSpegVAAnis",
        "outputId": "469ebce0-9371-4966-fae8-5be258c3d721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image \n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "# %debug\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "path = '/content/drive/My Drive/Skynet/WVY'\n",
        "path_out = '/content/drive/My Drive/Skynet/WVY_SCRUB'\n",
        "# listdir(path)\n",
        "\n",
        "\n",
        "\n",
        "# https://www.youtube.com/watch?v=2pQOXjpO_u0 creating our own dataset\n",
        "#current image mode for gif input is P, meaning 1 bytes per pixel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMk48Hikv6Td"
      },
      "source": [
        "### Run following 2 sections to load images from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEyCl5GPv4Bn",
        "outputId": "1d4fcc6d-a664-4334-9643-3961e27a0327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "files_in_dir = [f for f in listdir(path) if isfile(join(path, f)) if f.split('_')[4] == 'RAIN.gif']\n",
        "\n",
        "\n",
        "print(len(files_in_dir))\n",
        "# for f in files_in_dir:\n",
        "#     print(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLR_XJlEWjHC",
        "outputId": "fdf235cb-5fe0-428b-ddc7-08b7ee2f4d6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "numImages = 120*40\n",
        "background_image = plt.imread('background.gif')\n",
        "\n",
        "count = 0\n",
        "rain_images = []\n",
        "files_in_dir = files_in_dir[:numImages]  # Reduce number of images for now to 60\n",
        "\n",
        "\n",
        "for f in files_in_dir:  # Clean and crop each image\n",
        "    s = join(path, files_in_dir[count])\n",
        "    count += 1\n",
        "\n",
        "    rain_image = plt.imread(s)\n",
        "    height, width, depth = rain_image.shape\n",
        "\n",
        "    imgdiff = background_image - rain_image # Perform background suppression\n",
        "    imggood = cv2.cvtColor(imgdiff, cv2.COLOR_BGR2RGB) # Switch back to normal color encoding\n",
        "\n",
        "    cropped = imggood[0:480, 0:480]  # Crop\n",
        "    height, width, depth = cropped.shape\n",
        "    resized = cv2.resize(cropped, (int(height / 7.5), int(width / 7.5)), interpolation = cv2.INTER_AREA)  # Resize to 64 by 64\n",
        "    converted = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY)\n",
        "    converted = converted[..., np.newaxis]\n",
        "    rain_images.append(np.asarray(converted))  # Add to new rain images list\n",
        "\n",
        "# imshow(rain_images[0])  # Test output\n",
        "# half_images = numImages // 2\n",
        "half_images = math.floor(numImages * 0.8)\n",
        "train_list = rain_images[:half_images]  # Split into 30 train and 30 test\n",
        "test_list = rain_images[half_images:]\n",
        "\n",
        "print('done loading images and converting to greyscale')\n",
        "\n",
        "train_images = []\n",
        "train_images_extra = []\n",
        "train_labels = []\n",
        "for i in range (0, len(train_list)):  # Split train into 20 images and 10 labels\n",
        "  if i % 3 == 2:\n",
        "    train_labels.append(np.ndarray.flatten(train_list[i]))\n",
        "  elif i % 3 == 1:\n",
        "    train_images_extra.append(train_list[i])\n",
        "  else:\n",
        "    train_images.append(train_list[i])\n",
        "\n",
        "\n",
        "test_images = []\n",
        "test_images_extra = []\n",
        "test_labels = []\n",
        "for i in range (0, len(test_list)):  # Split test into 20 images and 10 labels\n",
        "  if i % 3 == 2:\n",
        "    test_labels.append(np.ndarray.flatten(test_list[i]))\n",
        "  elif i % 3 == 1:\n",
        "    test_images_extra.append(test_list[i])\n",
        "  else:\n",
        "    test_images.append(test_list[i])\n",
        "\n",
        "print('done separating training/testing images and labels')\n",
        "\n",
        "good_train_images = []\n",
        "for i in range(0, len(train_labels)):\n",
        "  image = np.concatenate((train_images[i], train_images_extra[i]), 0)\n",
        "  image = np.expand_dims(image, axis=0)\n",
        "  good_train_images.append(image)\n",
        "\n",
        "good_test_images = []\n",
        "for i in range(0, len(test_labels)):\n",
        "  image = np.concatenate((test_images[i], test_images_extra[i]), 0)\n",
        "  image = np.expand_dims(image, axis=0)\n",
        "  good_test_images.append(image)\n",
        "\n",
        "\n",
        "\n",
        "good_train_images = np.concatenate((good_train_images), 0)\n",
        "print(good_train_images.shape)\n",
        "\n",
        "train_labels = np.asarray(train_labels)\n",
        "print(train_labels.shape)\n",
        "\n",
        "good_test_images = np.concatenate((good_test_images), 0)\n",
        "print(good_test_images.shape)\n",
        "\n",
        "test_labels = np.asarray(test_labels)\n",
        "print(test_labels.shape)\n",
        "#train_labels = np.concatenate((train_labels), 0)\n",
        "\n",
        "\n",
        "#print(train_labels.shape)\n",
        "\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done loading images and converting to greyscale\n",
            "done separating training/testing images and labels\n",
            "(1280, 128, 64, 1)\n",
            "(1280, 4096)\n",
            "(320, 128, 64, 1)\n",
            "(320, 4096)\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQFHClY5r8so"
      },
      "source": [
        "#### Run this to save the current arrays as files - should make it faster to load the sets in the future"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtUdLOASq83f",
        "outputId": "1c55067a-7e35-4c64-cb5d-539261830557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.save('/content/drive/My Drive/Skynet/training_images.npy', good_train_images)\n",
        "np.save('/content/drive/My Drive/Skynet/training_labels.npy', train_labels)\n",
        "np.save('/content/drive/My Drive/Skynet/testing_images.npy', good_test_images)\n",
        "np.save('/content/drive/My Drive/Skynet/testing_labels.npy', test_labels)\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dz50yi4uGqh"
      },
      "source": [
        "## Only need to run this to load the training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07O16UEVtrZh",
        "outputId": "e4a059be-c75c-49f0-e8ea-5a2978e3233b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "good_train_images = np.load('/content/drive/My Drive/Skynet/training_images.npy')\n",
        "train_labels = np.load('/content/drive/My Drive/Skynet/training_labels.npy')\n",
        "good_test_images = np.load('/content/drive/My Drive/Skynet/testing_images.npy')\n",
        "test_labels = np.load('/content/drive/My Drive/Skynet/testing_labels.npy')\n",
        "\n",
        "print(good_train_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(good_test_images.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1280, 128, 64, 1)\n",
            "(1280, 4096)\n",
            "(320, 128, 64, 1)\n",
            "(320, 4096)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMY4pMIaQTdP",
        "outputId": "67b21bde-c387-4f9a-903f-130f479f0a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install varname\n",
        "\n",
        "from varname import nameof\n",
        "\n",
        "\n",
        "def print_shape(variable_in, name):\n",
        "    print('{} shape: {}'.format(name, variable_in.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: varname in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: executing in /usr/local/lib/python3.6/dist-packages (from varname) (0.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsSdM3xOT659"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQpVNvmCkSoJ"
      },
      "source": [
        "### Setting up model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFP8qCPdd4fG"
      },
      "source": [
        "\n",
        "    # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
        "def loss(y_true,y_pred):\n",
        "    # return K.mean(K.square(y_pred - y_true), axis=-1)\n",
        "    return (y_true - y_pred)**2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoImkN81BL6y",
        "outputId": "313eacfe-ba88-44b6-ec63-7218bb87eca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from tensorflow.python.keras import Model\n",
        "# code from here https://github.com/zhixuhao/unet\n",
        "#concatenate basically adds one image to the end of the other for each image the 2 layers (keeping each of the 2 added ones seperate)\n",
        "# https://keras.io/api/layers/merging_layers/concatenate/\n",
        "\n",
        "\n",
        "input1 = keras.Input((128, 64, 1))\n",
        "print_shape(input1, nameof(input1))\n",
        "\n",
        "# input2 = Input((64, 64, 1))\n",
        "# print_shape(input2, 'input 2')\n",
        "\n",
        "# inputs_avg = Average()([input1, input2])\n",
        "# inputs_avg = Concatenate()([input1, input2])\n",
        "\n",
        "#basic 1 start\n",
        "\n",
        "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input1)\n",
        "print_shape(conv1, nameof(conv1))\n",
        "\n",
        "batchNorm1 = BatchNormalization(axis=3)(conv1)\n",
        "print_shape(batchNorm1, 'batch norm 1')\n",
        "\n",
        "leakyRelu1 = LeakyReLU(alpha=0.3)(batchNorm1)\n",
        "print_shape(leakyRelu1, nameof(leakyRelu1))\n",
        "\n",
        "conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu1)\n",
        "print_shape(conv1, nameof(conv1))\n",
        "\n",
        "# basic 1 end\n",
        "\n",
        "#down-sample 1 start\n",
        "\n",
        "batchNorm2 = BatchNormalization(axis=3)(conv1)\n",
        "print_shape(batchNorm2, 'batch norm 2')\n",
        "\n",
        "leakyRelu2 = LeakyReLU(alpha=0.3)(batchNorm2)\n",
        "print_shape(leakyRelu2, nameof(leakyRelu2))\n",
        "\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2))(leakyRelu2)\n",
        "print_shape(pool1, nameof(pool1))\n",
        "\n",
        "batchNorm3 = BatchNormalization(axis=3)(pool1)\n",
        "print_shape(batchNorm3, 'batch norm 3')\n",
        "\n",
        "leakyRelu3 = LeakyReLU(alpha=0.3)(batchNorm3)\n",
        "print_shape(leakyRelu3, nameof(leakyRelu3))\n",
        "\n",
        "conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu3)\n",
        "print_shape(conv2, nameof(conv2))\n",
        "\n",
        "#down-sample 1 end\n",
        "#down-sample 2 start\n",
        "\n",
        "# conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "# print_shape(conv2, nameof(conv2))\n",
        "\n",
        "batchNorm4 = BatchNormalization(axis=3)(conv2)\n",
        "print_shape(batchNorm4, 'batch norm 4')\n",
        "\n",
        "leakyRelu4 = LeakyReLU(alpha=0.3)(batchNorm4)\n",
        "print_shape(leakyRelu4, nameof(leakyRelu4))\n",
        "\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2))(leakyRelu4)\n",
        "print_shape(pool2, nameof(pool2))\n",
        "\n",
        "batchNorm5 = BatchNormalization(axis=3)(pool2)\n",
        "print_shape(batchNorm5, 'batch norm 5')\n",
        "\n",
        "leakyRelu5 = LeakyReLU(alpha=0.3)(batchNorm5)\n",
        "print_shape(leakyRelu5, nameof(leakyRelu5))\n",
        "\n",
        "conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu5)\n",
        "print_shape(conv3, nameof(conv3))\n",
        "\n",
        "# down-sample 2 end\n",
        "# down-sample 3 start\n",
        "\n",
        "# conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "# print_shape(conv3, nameof(conv3))\n",
        "\n",
        "batchNorm6 = BatchNormalization(axis=3)(conv3)\n",
        "print_shape(batchNorm1, 'batch norm 1')\n",
        "\n",
        "leakyRelu6 = LeakyReLU(alpha=0.3)(batchNorm6)\n",
        "print_shape(leakyRelu6, nameof(leakyRelu6))\n",
        "\n",
        "pool3 = MaxPooling2D(pool_size=(2, 2))(leakyRelu6)\n",
        "print_shape(pool3, nameof(pool3))\n",
        "\n",
        "batchNorm7 = BatchNormalization(axis=3)(pool3)\n",
        "print_shape(batchNorm7, 'batch norm 7')\n",
        "\n",
        "leakyRelu7 = LeakyReLU(alpha=0.3)(batchNorm7)\n",
        "print_shape(leakyRelu7, nameof(leakyRelu7))\n",
        "\n",
        "conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu7)\n",
        "print_shape(conv4, nameof(conv4))\n",
        "\n",
        "# conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "# print_shape(conv4, nameof(conv4))\n",
        "\n",
        "# down-sample 3 end\n",
        "\n",
        "# down-sample 4 start\n",
        "\n",
        "# drop4 = Dropout(0.5)(conv4)\n",
        "# print_shape(drop4, nameof(drop4))\n",
        "\n",
        "batchNorm8 = BatchNormalization(axis=3)(conv4)\n",
        "print_shape(batchNorm1, 'batch norm 8')\n",
        "\n",
        "leakyRelu8 = LeakyReLU(alpha=0.3)(batchNorm8)\n",
        "print_shape(leakyRelu8, nameof(leakyRelu8))\n",
        "\n",
        "pool4 = MaxPooling2D(pool_size=(2, 2))(leakyRelu8)\n",
        "print_shape(pool4, nameof(pool4))\n",
        "\n",
        "batchNorm9 = BatchNormalization(axis=3)(pool4)\n",
        "print_shape(batchNorm9, 'batch norm 9')\n",
        "\n",
        "leakyRelu9 = LeakyReLU(alpha=0.3)(batchNorm9)\n",
        "print_shape(leakyRelu9, nameof(leakyRelu9))\n",
        "\n",
        "conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu9)\n",
        "print_shape(conv5, nameof(conv5))\n",
        "\n",
        "# down-sample 4 end\n",
        "\n",
        "# conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "# print_shape(conv5, nameof(conv5))\n",
        "\n",
        "# drop5 = Dropout(0.5)(conv5)\n",
        "# print_shape(drop5, nameof(drop5))\n",
        "\n",
        "# TODO: PUT BASIC LAYER HERE\n",
        "\n",
        "conv_basic1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "print_shape(conv_basic1, nameof(conv_basic1))\n",
        "\n",
        "batchNorm_basic = BatchNormalization(axis=3)(conv_basic1)\n",
        "print_shape(batchNorm_basic, 'batch norm _basic')\n",
        "\n",
        "leakyRelu_basic = LeakyReLU(alpha=0.3)(batchNorm_basic)\n",
        "print_shape(leakyRelu_basic, nameof(leakyRelu_basic))\n",
        "\n",
        "conv_basic2 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu_basic)\n",
        "print_shape(conv_basic2, nameof(conv_basic2))\n",
        "\n",
        "# up-sample 1 start\n",
        "\n",
        "upa6 = UpSampling2D(size = (2,2))(conv_basic2)\n",
        "print_shape(upa6, nameof(upa6))\n",
        "\n",
        "batchNorm10 = BatchNormalization(axis=3)(upa6)\n",
        "print_shape(batchNorm10, 'batch norm 10')\n",
        "\n",
        "leakyRelu10 = LeakyReLU(alpha=0.3)(batchNorm10)\n",
        "print_shape(leakyRelu10, nameof(leakyRelu10))\n",
        "\n",
        "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu10)\n",
        "print_shape(up6, nameof(up6))\n",
        "\n",
        "batchNorm11 = BatchNormalization(axis=3)(up6)\n",
        "print_shape(batchNorm11, 'batch norm 11')\n",
        "\n",
        "leakyRelu11 = LeakyReLU(alpha=0.3)(batchNorm11)\n",
        "print_shape(leakyRelu11, nameof(leakyRelu11))\n",
        "\n",
        "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu11)\n",
        "print_shape(up6, nameof(up6))\n",
        "\n",
        "# up-sample 1 end\n",
        "\n",
        "# up-sample 2 start\n",
        "\n",
        "merge6 = concatenate([conv4,up6], axis = 3)\n",
        "# merge6 = concatenate([drop4,up6], axis = 3)\n",
        "print_shape(merge6, nameof(merge6))\n",
        "\n",
        "# conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "# print_shape(conv6, nameof(conv6))\n",
        "\n",
        "upa7 = UpSampling2D(size = (2,2))(merge6)\n",
        "print_shape(upa7, nameof(upa7))\n",
        "\n",
        "batchNorm12 = BatchNormalization(axis=3)(upa7)\n",
        "print_shape(batchNorm12, 'batch norm 12')\n",
        "\n",
        "leakyRelu12 = LeakyReLU(alpha=0.3)(batchNorm12)\n",
        "print_shape(leakyRelu12, nameof(leakyRelu12))\n",
        "\n",
        "\n",
        "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu12)\n",
        "print_shape(up7, nameof(up7))\n",
        "\n",
        "batchNorm13 = BatchNormalization(axis=3)(up7)\n",
        "print_shape(batchNorm13, 'batch norm 13')\n",
        "\n",
        "leakyRelu13 = LeakyReLU(alpha=0.3)(batchNorm13)\n",
        "print_shape(leakyRelu13, nameof(leakyRelu13))\n",
        "\n",
        "up7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu13)\n",
        "print_shape(up7, nameof(up7))\n",
        "\n",
        "# up-sample 2 end\n",
        "\n",
        "merge7 = concatenate([conv3,up7], axis = 3)\n",
        "print_shape(merge7, nameof(merge7))\n",
        "\n",
        "# up-sample 3 start\n",
        "\n",
        "\n",
        "\n",
        "# conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "# print_shape(conv7, nameof(conv7))\n",
        "\n",
        "upa8 = UpSampling2D(size = (2,2))(merge7)\n",
        "\n",
        "batchNorm14 = BatchNormalization(axis=3)(upa8)\n",
        "print_shape(batchNorm14, 'batch norm 14')\n",
        "\n",
        "leakyRelu14 = LeakyReLU(alpha=0.3)(batchNorm14)\n",
        "print_shape(leakyRelu14, nameof(leakyRelu14))\n",
        "\n",
        "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu14)\n",
        "print_shape(up8, nameof(up8))\n",
        "\n",
        "batchNorm15 = BatchNormalization(axis=3)(up8)\n",
        "print_shape(batchNorm15, 'batch norm 15')\n",
        "\n",
        "leakyRelu15 = LeakyReLU(alpha=0.3)(batchNorm15)\n",
        "print_shape(leakyRelu15, nameof(leakyRelu15))\n",
        "\n",
        "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu15)\n",
        "print_shape(up8, nameof(up8))\n",
        "\n",
        "# up-sample 3 end\n",
        "# up-sample 4 start\n",
        "\n",
        "merge8 = concatenate([conv2,up8], axis = 3)\n",
        "print_shape(merge8, nameof(merge8))\n",
        "\n",
        "# conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "# print_shape(conv8, nameof(conv8))\n",
        "\n",
        "# conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "# print_shape(conv8, nameof(conv8))\n",
        "\n",
        "upa9 = UpSampling2D(size = (2,2))(merge8)\n",
        "\n",
        "batchNorm13 = BatchNormalization(axis=3)(upa9)\n",
        "print_shape(batchNorm13, 'batch norm 13')\n",
        "\n",
        "leakyRelu13 = LeakyReLU(alpha=0.3)(batchNorm13)\n",
        "print_shape(leakyRelu13, nameof(leakyRelu13))\n",
        "\n",
        "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu13)\n",
        "print_shape(up9, nameof(up9))\n",
        "\n",
        "batchNorm13 = BatchNormalization(axis=3)(up9)\n",
        "print_shape(batchNorm13, 'batch norm 13')\n",
        "\n",
        "leakyRelu13 = LeakyReLU(alpha=0.3)(batchNorm13)\n",
        "print_shape(leakyRelu13, nameof(leakyRelu13))\n",
        "\n",
        "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu13)\n",
        "print_shape(up9, nameof(up9))\n",
        "\n",
        "merge9 = concatenate([conv1,up9], axis = 3)\n",
        "print_shape(merge9, nameof(merge9))\n",
        "\n",
        "# BASIC FROM HERE ON\n",
        "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "print_shape(conv9, nameof(conv9))\n",
        "\n",
        "batchNorm14 = BatchNormalization(axis=3)(conv9)\n",
        "print_shape(batchNorm13, 'batch norm 13')\n",
        "\n",
        "leakyRelu14 = LeakyReLU(alpha=0.3)(batchNorm14)\n",
        "print_shape(leakyRelu14, nameof(leakyRelu14))\n",
        "\n",
        "conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(leakyRelu14)\n",
        "print_shape(conv9, nameof(conv9))\n",
        "\n",
        "conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "print_shape(conv9, nameof(conv9))\n",
        "\n",
        "conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "print_shape(conv10, nameof(conv10))\n",
        "print('conv 10 shape: ', conv10.shape)\n",
        "\n",
        "\n",
        "flated = Flatten()(conv10)\n",
        "print('flat shape: ', flated.shape)\n",
        "\n",
        "dense_penultimate = Dense(4096, activation='relu')(flated)\n",
        "print_shape(dense_penultimate, nameof(dense_penultimate))\n",
        "\n",
        "# dense_end = Dense(8)(dense_penultimate) # 8 classes at end, one for each rain colour amount\n",
        "# print_shape(dense_end, nameof(dense_end))\n",
        "\n",
        "\n",
        "model_online = Model(inputs = input1, outputs = dense_penultimate)\n",
        "\n",
        "# model_online.compile(optimizer ='adamax', loss = loss, metrics = ['accuracy', 'poisson'])\n",
        "model_online.compile(optimizer ='adamax', loss = 'binary-crossentropy', metrics = ['accuracy', 'poisson'])\n",
        "\n",
        "model_online.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input1 shape: (?, 128, 64, 1)\n",
            "conv1 shape: (?, 128, 64, 64)\n",
            "batch norm 1 shape: (?, 128, 64, 64)\n",
            "leakyRelu1 shape: (?, 128, 64, 64)\n",
            "conv1 shape: (?, 128, 64, 64)\n",
            "batch norm 2 shape: (?, 128, 64, 64)\n",
            "leakyRelu2 shape: (?, 128, 64, 64)\n",
            "pool1 shape: (?, 64, 32, 64)\n",
            "batch norm 3 shape: (?, 64, 32, 64)\n",
            "leakyRelu3 shape: (?, 64, 32, 64)\n",
            "conv2 shape: (?, 64, 32, 128)\n",
            "batch norm 4 shape: (?, 64, 32, 128)\n",
            "leakyRelu4 shape: (?, 64, 32, 128)\n",
            "pool2 shape: (?, 32, 16, 128)\n",
            "batch norm 5 shape: (?, 32, 16, 128)\n",
            "leakyRelu5 shape: (?, 32, 16, 128)\n",
            "conv3 shape: (?, 32, 16, 256)\n",
            "batch norm 1 shape: (?, 128, 64, 64)\n",
            "leakyRelu6 shape: (?, 32, 16, 256)\n",
            "pool3 shape: (?, 16, 8, 256)\n",
            "batch norm 7 shape: (?, 16, 8, 256)\n",
            "leakyRelu7 shape: (?, 16, 8, 256)\n",
            "conv4 shape: (?, 16, 8, 512)\n",
            "batch norm 8 shape: (?, 128, 64, 64)\n",
            "leakyRelu8 shape: (?, 16, 8, 512)\n",
            "pool4 shape: (?, 8, 4, 512)\n",
            "batch norm 9 shape: (?, 8, 4, 512)\n",
            "leakyRelu9 shape: (?, 8, 4, 512)\n",
            "conv5 shape: (?, 8, 4, 1024)\n",
            "conv_basic1 shape: (?, 8, 4, 64)\n",
            "batch norm _basic shape: (?, 8, 4, 64)\n",
            "leakyRelu_basic shape: (?, 8, 4, 64)\n",
            "conv_basic2 shape: (?, 8, 4, 64)\n",
            "upa6 shape: (?, 16, 8, 64)\n",
            "batch norm 10 shape: (?, 16, 8, 64)\n",
            "leakyRelu10 shape: (?, 16, 8, 64)\n",
            "up6 shape: (?, 16, 8, 512)\n",
            "batch norm 11 shape: (?, 16, 8, 512)\n",
            "leakyRelu11 shape: (?, 16, 8, 512)\n",
            "up6 shape: (?, 16, 8, 512)\n",
            "merge6 shape: (?, 16, 8, 1024)\n",
            "upa7 shape: (?, 32, 16, 1024)\n",
            "batch norm 12 shape: (?, 32, 16, 1024)\n",
            "leakyRelu12 shape: (?, 32, 16, 1024)\n",
            "up7 shape: (?, 32, 16, 256)\n",
            "batch norm 13 shape: (?, 32, 16, 256)\n",
            "leakyRelu13 shape: (?, 32, 16, 256)\n",
            "up7 shape: (?, 32, 16, 256)\n",
            "merge7 shape: (?, 32, 16, 512)\n",
            "batch norm 14 shape: (?, 64, 32, 512)\n",
            "leakyRelu14 shape: (?, 64, 32, 512)\n",
            "up8 shape: (?, 64, 32, 128)\n",
            "batch norm 15 shape: (?, 64, 32, 128)\n",
            "leakyRelu15 shape: (?, 64, 32, 128)\n",
            "up8 shape: (?, 64, 32, 128)\n",
            "merge8 shape: (?, 64, 32, 256)\n",
            "batch norm 13 shape: (?, 128, 64, 256)\n",
            "leakyRelu13 shape: (?, 128, 64, 256)\n",
            "up9 shape: (?, 128, 64, 64)\n",
            "batch norm 13 shape: (?, 128, 64, 64)\n",
            "leakyRelu13 shape: (?, 128, 64, 64)\n",
            "up9 shape: (?, 128, 64, 64)\n",
            "merge9 shape: (?, 128, 64, 128)\n",
            "conv9 shape: (?, 128, 64, 64)\n",
            "batch norm 13 shape: (?, 128, 64, 64)\n",
            "leakyRelu14 shape: (?, 128, 64, 64)\n",
            "conv9 shape: (?, 128, 64, 64)\n",
            "conv9 shape: (?, 128, 64, 2)\n",
            "conv10 shape: (?, 128, 64, 1)\n",
            "conv 10 shape:  (?, 128, 64, 1)\n",
            "flat shape:  (?, 8192)\n",
            "dense_penultimate shape: (?, 4096)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 128, 64, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 128, 64, 64)  640         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 128, 64, 64)  256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 128, 64, 64)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 128, 64, 64)  36928       leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 128, 64, 64)  256         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 128, 64, 64)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 64, 32, 64)   0           leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 64, 32, 64)   256         max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 64, 32, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 64, 32, 128)  73856       leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 64, 32, 128)  512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 64, 32, 128)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 32, 16, 128)  0           leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 32, 16, 128)  512         max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_23 (LeakyReLU)      (None, 32, 16, 128)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 32, 16, 256)  295168      leaky_re_lu_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 32, 16, 256)  1024        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_24 (LeakyReLU)      (None, 32, 16, 256)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 16, 8, 256)   0           leaky_re_lu_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 8, 256)   1024        max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_25 (LeakyReLU)      (None, 16, 8, 256)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 8, 512)   1180160     leaky_re_lu_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 8, 512)   2048        conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_26 (LeakyReLU)      (None, 16, 8, 512)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 8, 4, 512)    0           leaky_re_lu_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 4, 512)    2048        max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_27 (LeakyReLU)      (None, 8, 4, 512)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 4, 1024)   4719616     leaky_re_lu_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 4, 64)     589888      conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 4, 64)     256         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_28 (LeakyReLU)      (None, 8, 4, 64)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 4, 64)     36928       leaky_re_lu_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 16, 8, 64)    0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 8, 64)    256         up_sampling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_29 (LeakyReLU)      (None, 16, 8, 64)    0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 8, 512)   131584      leaky_re_lu_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 8, 512)   2048        conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, 16, 8, 512)   0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 16, 8, 512)   1049088     leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 16, 8, 1024)  0           conv2d_24[0][0]                  \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 32, 16, 1024) 0           concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 32, 16, 1024) 4096        up_sampling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, 32, 16, 1024) 0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 32, 16, 256)  1048832     leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 32, 16, 256)  1024        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 32, 16, 256)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 32, 16, 256)  590080      leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 16, 512)  0           conv2d_23[0][0]                  \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 64, 32, 512)  0           concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 64, 32, 512)  2048        up_sampling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 64, 32, 512)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 64, 32, 128)  262272      leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 64, 32, 128)  512         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 64, 32, 128)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 64, 32, 128)  65664       leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 64, 32, 256)  0           conv2d_22[0][0]                  \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 128, 64, 256) 0           concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 128, 64, 256) 1024        up_sampling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)      (None, 128, 64, 256) 0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 128, 64, 64)  65600       leaky_re_lu_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 128, 64, 64)  256         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)      (None, 128, 64, 64)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 128, 64, 64)  16448       leaky_re_lu_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 128, 64, 128) 0           conv2d_21[0][0]                  \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 128, 64, 64)  73792       concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 128, 64, 64)  256         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)      (None, 128, 64, 64)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 128, 64, 64)  36928       leaky_re_lu_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 128, 64, 2)   1154        conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 128, 64, 1)   3           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 8192)         0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4096)         33558528    flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 43,852,869\n",
            "Trainable params: 43,843,013\n",
            "Non-trainable params: 9,856\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WzxrxOOkV28"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yTG4bOk1wZz",
        "outputId": "c2fc0ccf-8416-4121-885a-eef5e243c53c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!rm -rf ./logs/ \n",
        "\n",
        "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "\n",
        "\n",
        "# train_images = np.stack(train_images, axis=0)\n",
        "# train_labels = np.stack(train_labels, axis=0)\n",
        "\n",
        "model_online.fit(\n",
        "    good_train_images,\n",
        "    train_labels, \n",
        "    batch_size=16,\n",
        "    epochs=3,\n",
        "    verbose=1,\n",
        "    validation_split = 0.1,\n",
        "    callbacks=[tensorboard_callback])\n",
        "\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1152 samples, validate on 128 samples\n",
            "Epoch 1/3\n",
            "1152/1152 [==============================] - 995s 864ms/sample - loss: 14077.2784 - acc: 8.6806e-04 - poisson: 655.7801 - val_loss: 11684.6587 - val_acc: 0.0000e+00 - val_poisson: 545.7925\n",
            "Epoch 2/3\n",
            "1152/1152 [==============================] - 981s 852ms/sample - loss: 10278.4699 - acc: 0.0000e+00 - poisson: 528.6441 - val_loss: 11102.7646 - val_acc: 0.0000e+00 - val_poisson: 520.9187\n",
            "Epoch 3/3\n",
            " 992/1152 [========================>.....] - ETA: 2:13 - loss: 10129.4240 - acc: 0.0000e+00 - poisson: 517.6982"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}